# Awesome-LLM

Large Language Models (LLMs) are foundational machine learning models to process and understand natural language. These models are trained on massive amounts of text data to learn patterns in the language. LLMs can perform many types of language tasks, such as translating languages, analyzing sentiments, chatbot, conversational agents, and much more.



## Frameworks

* [transformers](https://github.com/huggingface/text-generation-inference): ðŸ¤— Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models.
* [datasets](https://github.com/huggingface/datasets): ðŸ¤—Â The largest hub of ready-to-use datasets for ML models with fast, easy-to-use and efficient data manipulation tools.

### Training

* [accelerate](https://github.com/huggingface/accelerate): ðŸš€ A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.
* [optimum](https://github.com/huggingface/optimum): ðŸš€Â Accelerate training and inference ofÂ ðŸ¤—Â Transformers andÂ ðŸ¤—Â Diffusers with easy to use hardware optimization tools.

### Serving

* [text-generation-inference](https://github.com/huggingface/text-generation-inference): A Rust, Python and gRPC server for text generation inference. Used in production atÂ [HuggingFace](https://huggingface.co/)Â to power LLMs api-inference widgets.
* [vLLM](https://github.com/vllm-project/vllm) :A high-throughput and memory-efficient inference and serving engine for LLMs.
* [Basaran](https://github.com/hyperonym/basaran): Basaran is an open-source alternative to the OpenAI text completion API. It provides a compatible streaming API for your Hugging Face Transformers-based text generation models.

### Quantization

* [PEFT](https://github.com/huggingface/peft): ðŸ¤—Parameter-Efficient-Fine-Tuning methods enables efficient adaption of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. 
* [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ): An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.
* [ExLLama](https://github.com/turboderp/exllama): A standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.
* [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa): 4 bits quantization of LLaMA using GPTQ.

## Tutorials



## Blogs/Articles

* https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/



